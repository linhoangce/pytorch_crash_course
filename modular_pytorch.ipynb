{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNKeAOMU06j/lLhavBRz58Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/linhoangce/pytorch_crash_course/blob/main/modular_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Get Data\n"
      ],
      "metadata": {
        "id": "bjp_0VU__sSv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "\n",
        "# Setup path to data folder\n",
        "data_path = Path('data')\n",
        "image_path = data_path / 'pizza_steak_sushi'\n",
        "\n",
        "# check if folder exists before downloading\n",
        "if image_path.is_dir():\n",
        "  print(f'{image_path} exists.')\n",
        "else:\n",
        "  print(f\"{image_path} doesn't exist. Creating folder...\")\n",
        "  image_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# download data\n",
        "with open(data_path / 'pizza_steak_sushi.zip', 'wb') as f:\n",
        "  request = requests.get('https://github.com/mrdbourke/pytorch-deep-learning/raw/refs/heads/main/data/pizza_steak_sushi.zip')\n",
        "  print(f'Downloading data from {request.url}...')\n",
        "  f.write(request.content)\n",
        "\n",
        "# Unzip data\n",
        "with zipfile.ZipFile(data_path / 'pizza_steak_sushi.zip', 'r') as zip_ref:\n",
        "  print(\"Unzipping data ...\")\n",
        "  zip_ref.extractall(image_path)\n",
        "\n",
        "# Remove zip file\n",
        "os.remove(data_path / \"pizza_steak_sushi.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbnUI7VHKmJf",
        "outputId": "b2dfef73-f732-40ae-ea67-27a53862aea7"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data/pizza_steak_sushi exists.\n",
            "Downloading data from https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/refs/heads/main/data/pizza_steak_sushi.zip...\n",
            "Unzipping data ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Create Datasets and DataLoaders (`data_setup.py`)"
      ],
      "metadata": {
        "id": "bz_CCNvbMbbm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create module folder\n",
        "# module_path = Path('going_module')\n",
        "\n",
        "# if not module_path.is_dir():\n",
        "#   module_path.mkdir(parents=True, exist_ok=True)\n",
        "import os\n",
        "\n",
        "if not os.path.exists('going_modular'):\n",
        "  os.makedirs('going_modular')"
      ],
      "metadata": {
        "id": "WuJEJEACPC5r"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile going_modular/data_setup.py\n",
        "\"\"\"\n",
        "Contains functionality for creating PyTorch DataLoaders for image classification data.\n",
        "\"\"\"\n",
        "import os\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "NUM_WORKERS = os.cpu_count()\n",
        "\n",
        "def create_dataloaders(\n",
        "    train_dir: str,\n",
        "    test_dir: str,\n",
        "    transform: transforms.Compose,\n",
        "    batch_size: int,\n",
        "    num_workers: int=NUM_WORKERS\n",
        "):\n",
        "  \"\"\"Creates training and testing DataLoaders.\n",
        "\n",
        "  Takes in a training and testing directory path and turns them into PyTorch Datasets and then into PyTorch DataLoaders.\n",
        "\n",
        "  Args:\n",
        "    train_dir: Path to training directory.\n",
        "    test_dir: Path to testing directory.\n",
        "    transform: torchvision transforms to perform on training and testing data.\n",
        "    batch_size: Number of samples per batch in each of the DataLoaders.\n",
        "    num_workers: An integer for number of workers per DataLoader.\n",
        "\n",
        "  Returns:\n",
        "    A tuple of (train_dataloader, test_dataloader, class_names).\n",
        "    Where class_names is a list of the target classes.\n",
        "\n",
        "  Example usage:\n",
        "    train_dataloader, test_dataloader, class_names = create_dataloaders(\n",
        "      train_dir=path/to/train_dir,\n",
        "      test_dir=path/to/test_dir,\n",
        "      trainsform=some_transform,\n",
        "      batch_size=32,\n",
        "      num_workers=3\n",
        "    )\n",
        "  \"\"\"\n",
        "  # Use ImageFolder to create dataset(s)\n",
        "  train_data = datasets.ImageFolder(root=train_dir, transform=transform)\n",
        "  test_data = datasets.ImageFolder(root=test_dir, transform=transform)\n",
        "\n",
        "  # Get the class names\n",
        "  class_names = train_data.classes\n",
        "\n",
        "  # Turn images into DataLoaders\n",
        "  train_dataloader = DataLoader(train_data,\n",
        "                                batch_size=batch_size,\n",
        "                                shuffle=True,\n",
        "                                num_workers=num_workers,\n",
        "                                pin_memory=True)\n",
        "\n",
        "  test_dataloader = DataLoader(test_data,\n",
        "                               batch_size=batch_size,\n",
        "                               shuffle=False,\n",
        "                               num_workers=num_workers,\n",
        "                               pin_memory=True)\n",
        "\n",
        "  return train_dataloader, test_dataloader, class_names\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7k90lz_MoWN",
        "outputId": "7e9455d2-5e56-4603-91f6-214e8ac3c552"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting going_modular/data_setup.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from going_modular import data_setup"
      ],
      "metadata": {
        "id": "LOUyS0AmO8WO"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = image_path / 'train'\n",
        "test_dir = image_path / 'test'"
      ],
      "metadata": {
        "id": "U80QUak97K_8"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "data_transform = transforms.Compose([\n",
        "    transforms.Resize(size=(64, 64)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.ToTensor()\n",
        "])"
      ],
      "metadata": {
        "id": "yVPsmpv17WXX"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n",
        "    train_dir=train_dir,\n",
        "    test_dir=test_dir,\n",
        "    transform=data_transform,\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "train_dataloader, test_dataloader, class_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DShebz1f5EBU",
        "outputId": "94178fc5-92c6-49f0-8cc5-f62e6466862c"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<torch.utils.data.dataloader.DataLoader at 0x7bdbe5f4eb10>,\n",
              " <torch.utils.data.dataloader.DataLoader at 0x7bdcbe7d3410>,\n",
              " ['pizza', 'steak', 'sushi'])"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Create a model builder script"
      ],
      "metadata": {
        "id": "EHRaykZ47nif"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf going_modular/model_builder.py"
      ],
      "metadata": {
        "id": "2ex6IQOWIQGZ"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile going_modular/model_builder.py\n",
        "\"\"\"\n",
        "Contains PyTorch model code to instantiate a TinyVGG model.\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class TinyVGG(nn.Module):\n",
        "  \"\"\"Creates the TinyVGG architecture.\n",
        "\n",
        "  Replicates the TinyVGG architecture from the CNN Explainer website in PyTorch.\n",
        "  See the original architecture here: https://poloclub.github.io/cnn-explainer/\n",
        "\n",
        "  Args:\n",
        "    input_shape: An integer indicating number of input channels.\n",
        "    hidden_units: An integer indicating number of hidden units between layers.\n",
        "    output_shape: An integer indicating number of output units.\n",
        "  \"\"\"\n",
        "  def __init__(self,\n",
        "               input_shape,\n",
        "               hidden_units,\n",
        "               output_shape):\n",
        "    super().__init__()\n",
        "    self.conv_block1 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=input_shape,\n",
        "                  out_channels=hidden_units,\n",
        "                  kernel_size=3,\n",
        "                  stride=1,\n",
        "                  padding=0),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels=hidden_units,\n",
        "                  out_channels=hidden_units,\n",
        "                  kernel_size=3,\n",
        "                  stride=1,\n",
        "                  padding=0),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2,\n",
        "                     stride=2)\n",
        "    )\n",
        "    self.conv_block2 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=hidden_units,\n",
        "                  out_channels=hidden_units,\n",
        "                  kernel_size=3,\n",
        "                  stride=1,\n",
        "                  padding=0),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels=hidden_units,\n",
        "                  out_channels=hidden_units,\n",
        "                  kernel_size=3,\n",
        "                  stride=1,\n",
        "                  padding=0),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2,\n",
        "                     stride=2)\n",
        "    )\n",
        "    self.classifier = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(in_features=hidden_units*13*13,\n",
        "                  out_features=output_shape)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    # x = self.conv_block1(x)\n",
        "    # print(x.shape)\n",
        "    # x = self.conv_block2(x)\n",
        "    # print(x.shape)\n",
        "    # x = self.classifier(x)\n",
        "    # return x\n",
        "    return self.classifier(self.conv_block2(self.conv_block1(x)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZhJxS7L_8goj",
        "outputId": "7c4d6322-e889-417b-a8c8-f8d6971830b4"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing going_modular/model_builder.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from going_modular import model_builder\n",
        "\n",
        "model_0 = model_builder.TinyVGG(\n",
        "    input_shape=3,\n",
        "    hidden_units=10,\n",
        "    output_shape=len(class_names)\n",
        ")"
      ],
      "metadata": {
        "id": "C64ugTIIIqcG"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# 1. Get a batch of images and labels from the DataLoader\n",
        "img_batch, label_batch = next(iter(train_dataloader))\n",
        "\n",
        "# 2, Get a single image from the batch and unsqueeze the image so its shape fits the model\n",
        "img_single, label_single = img_batch[0].unsqueeze(dim=0), label_batch[0]\n",
        "print(f\"Single image shape: {img_single.shape}\")\n",
        "\n",
        "# 3. Perform a forward pass os a single image\n",
        "model_0.eval()\n",
        "with torch.inference_mode():\n",
        "  logits = model_0(img_single.to(device))\n",
        "\n",
        "print(f\"Output logits: \\n{logits}\\n\")\n",
        "print(f\"Pred probs: \\n{torch.softmax(logits, dim=1)}\\n\")\n",
        "print(f\"Pred label: \\n{torch.argmax(logits, dim=1)}\\n\")\n",
        "print(f\"True label: \\n{label_single}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6X0tIWnI_O6G",
        "outputId": "07e457e4-41af-43ae-8ec3-dafe013f045a"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Single image shape: torch.Size([1, 3, 64, 64])\n",
            "Output logits: \n",
            "tensor([[-0.0465, -0.0021,  0.0047]])\n",
            "\n",
            "Pred probs: \n",
            "tensor([[0.3228, 0.3375, 0.3397]])\n",
            "\n",
            "Pred label: \n",
            "tensor([2])\n",
            "\n",
            "True label: \n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Turn training function into a script"
      ],
      "metadata": {
        "id": "IaqTMj1EHwxZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile going_modular/engine.py\n",
        "\"\"\"\n",
        "Contains functions for training and testing a PyTorch model.\n",
        "\"\"\"\n",
        "import torch\n",
        "\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "\n",
        "def train_step(model: torch.nn.Module,\n",
        "               data_loader: torch.utils.data.DataLoader,\n",
        "               loss_fn: torch.nn.Module,\n",
        "               optimizer: torch.optim.Optimizer,\n",
        "               device: torch.device):\n",
        "  \"\"\"Trains a PyTorch model for a single epoch.\n",
        "\n",
        "  Turns a target PyTorch model to training mode and then\n",
        "  runs through all of the required training steps (forwrd pass,\n",
        "  loss calculation, optimizer step).\n",
        "\n",
        "  Args:\n",
        "    model: A PyTorch model to be trained.\n",
        "    data_loader: A DataLoader instance for the model to be trained on.\n",
        "    loss_fn: A PyTorch loss function to minimize.\n",
        "    optimizer: A  PyTorch optimizer to help minimize the loss.\n",
        "    device: A target device to compute on (e.g 'cuda' o 'cpu')\n",
        "\n",
        "  Returns:\n",
        "    A tuple of training loss and training accuracy metrics.\n",
        "    In the form (train_loss, train_accuracy). For example:\n",
        "\n",
        "    (0.1566, 0.8762)\n",
        "  \"\"\"\n",
        "  # Move model to target device\n",
        "  model.to(device)\n",
        "\n",
        "  # Put model in train mode\n",
        "  model.train()\n",
        "\n",
        "  # Setup train loss and accuracy values\n",
        "  train_loss, train_acc = 0, 0\n",
        "\n",
        "  # Loop through data loader data batches\n",
        "  for batch, (X, y) in enumerate(data_loader):\n",
        "    # Send data to target device\n",
        "    X, y = X.to(device), y.to(device)\n",
        "\n",
        "    # forward pass\n",
        "    logits = model(X)\n",
        "    y_pred = torch.argmax(logits, dim=1)\n",
        "\n",
        "    # calculate loss\n",
        "    loss = loss_fn(logits, y)\n",
        "    train_loss += loss.item()\n",
        "\n",
        "    # Optimizer zero grad\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # backward pass\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    # Calculate and accumulate accuracy metric across all batches\n",
        "    train_acc += (y_pred==y).sum().item() / len(y_pred)\n",
        "\n",
        "  # Calculate average loss and acc per batch\n",
        "  train_loss /= len(data_loader)\n",
        "  train_acc /= len(data_loader)\n",
        "\n",
        "  return train_loss, train_acc\n",
        "\n",
        "def test_step(model: torch.nn.Module,\n",
        "              data_loader: torch.utils.data.DataLoader,\n",
        "              loss_fn: torch.nn.Module,\n",
        "              device: torch.device):\n",
        "  \"\"\"Tests a PyTorch model for a single epoch.\n",
        "\n",
        "  Turns a target PyTorch model to \"eval\" mode and then performs\n",
        "  a forward pass on a testing dataset.\n",
        "\n",
        "  Args:\n",
        "    model: A PyTorch model to be tested.\n",
        "    data_loader: A DataLoader instance for the model to be tested on.\n",
        "    loss_fn: A PyTorch loss function to calculate loss on the test data.\n",
        "    device: A target device to compute on\n",
        "\n",
        "  Returns:\n",
        "  A tuple of testing loss and testing accuracy metrics.\n",
        "  In the form (test_loss, test_accuracy). For example:\n",
        "\n",
        "  (0.2211, 0.8877)\n",
        "  \"\"\"\n",
        "  # Move model to target device\n",
        "  model.to(device)\n",
        "\n",
        "  # Set up loss and acc\n",
        "  test_loss, test_acc = 0, 0\n",
        "\n",
        "  # Turn on eval mode\n",
        "  model.eval()\n",
        "  with torch.inference_mode():\n",
        "    for X, y in data_loader:\n",
        "      # move data to target device\n",
        "      X, y = X.to(device), y.to(device)\n",
        "\n",
        "      # forward pass\n",
        "      logits = model(X)\n",
        "      y_pred = torch.argmax(logits, dim=1)\n",
        "\n",
        "      loss = loss_fn(logits, y)\n",
        "      test_loss += loss.item()\n",
        "      test_acc += (y_pred==y).sum().item() / len(y_pred)\n",
        "\n",
        "  # calculate average loss and acc per batch\n",
        "  test_loss /= len(data_loader)\n",
        "  test_acc /= len(data_loader)\n",
        "\n",
        "  return test_loss, test_acc\n",
        "\n",
        "\n",
        "def train(model: torch.nn.Module,\n",
        "          train_dataloader: torch.utils.data.DataLoader,\n",
        "          test_dataloader: torch.utils.data.DataLoader,\n",
        "          loss_fn: torch.nn.Module,\n",
        "          optimizer: torch.optim.Optimizer,\n",
        "          epochs: int,\n",
        "          device: torch.device) -> Dict[str, List]:\n",
        "  \"\"\"Trains and tests a PyTorch model.\n",
        "\n",
        "  Passes a target PyTorch model through train_step() and test_step()\n",
        "  functions for a number of epochs, training and testing the model\n",
        "  in the same epoch loop.\n",
        "\n",
        "  Calculates, prints and stores evulation metrics throughout.\n",
        "\n",
        "  Args:\n",
        "    model: A PyTorch model to be trained and tested.\n",
        "    train_dataloader: A DataLoader instance for the model to be trained on.\n",
        "    test_dataloader: A DataLoader instance for the model to be tested on.\n",
        "    loss_fn: A PyTorch loss function to calculate loss on both datasets.\n",
        "    epochs: An integer indicating how many epochs to train for.\n",
        "    device: A target device to compute on.\n",
        "\n",
        "  Returns:\n",
        "    A dictionary of training and testing loss as well as training and\n",
        "    testing accuracy metrics. Each metric has a value in a list for each epoch.\n",
        "    In the form: {train_loss: [...],\n",
        "                  train_acc: [...],\n",
        "                  test_loss: [...],\n",
        "                  test_acc: [...]}\n",
        "    For example, if training for epochs=2:\n",
        "                  {train_loss: [2.0616, 1.0537],\n",
        "                  train_acc: [0.3945, 0.3945],\n",
        "                  test_loss: [1.2641, 1.5706],\n",
        "                  test_acc: [0.3400, 0.2973]}\n",
        "  \"\"\"\n",
        "  # create empty result dict\n",
        "  results = {\"train_loss\": [],\n",
        "             \"train_acc\": [],\n",
        "             \"test_loss\": [],\n",
        "             \"test_acc\": []}\n",
        "\n",
        "  # Loop thorugh training and testing steps for a number of epochs\n",
        "  for epoch in range(epochs):\n",
        "    train_loss, train_acc = train_step(model=model,\n",
        "                                       data_loader=train_dataloader,\n",
        "                                       loss_fn=loss_fn,\n",
        "                                       optimizer=optimizer,\n",
        "                                       device=device)\n",
        "    test_loss, test_acc = test_step(model=model,\n",
        "                                    data_loader=test_dataloader,\n",
        "                                    loss_fn=loss_fn,\n",
        "                                    device=device)\n",
        "\n",
        "    print(f\"Epoch: {epoch+1} | \"\n",
        "          f\"train_loss: {train_loss:.4f} | \"\n",
        "          f\"train_acc: {train_acc:.4f} | \"\n",
        "          f\"test_loss: {test_loss:.4f} | \"\n",
        "          f\"test_acc: {test_acc:.4f}\"\n",
        "          )\n",
        "\n",
        "    # Update result dict\n",
        "    results[\"train_loss\"].append(train_loss)\n",
        "    results[\"train_acc\"].append(train_acc)\n",
        "    results['test_loss'].append(test_loss)\n",
        "    results['test_acc'].append(test_acc)\n",
        "\n",
        "  return results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VKVEgcI5X8ZB",
        "outputId": "fd5135c3-d064-44be-a118-9da54e42000a"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting going_modular/engine.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from going_modular import engine"
      ],
      "metadata": {
        "id": "OgNT7AazeqNV"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiate loss and optimizer\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(params=model_0.parameters(),\n",
        "                             lr=0.001)"
      ],
      "metadata": {
        "id": "mp69PW3ee52Q"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 20\n",
        "\n",
        "engine.train(model=model_0,\n",
        "             train_dataloader=train_dataloader,\n",
        "             test_dataloader=test_dataloader,\n",
        "             loss_fn=loss_fn,\n",
        "             optimizer=optimizer,\n",
        "             epochs=EPOCHS,\n",
        "             device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZJIvaOffGp1",
        "outputId": "4c03fa50-ea0b-4c64-ea42-73ce481f956e"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 | train_loss: 1.1073 | train_acc: 0.2812 | test_loss: 1.0872 | test_acc: 0.5417\n",
            "Epoch: 2 | train_loss: 1.1030 | train_acc: 0.2969 | test_loss: 1.1063 | test_acc: 0.2604\n",
            "Epoch: 3 | train_loss: 1.0925 | train_acc: 0.4258 | test_loss: 1.0999 | test_acc: 0.2604\n",
            "Epoch: 4 | train_loss: 1.0997 | train_acc: 0.3047 | test_loss: 1.0983 | test_acc: 0.2604\n",
            "Epoch: 5 | train_loss: 1.0966 | train_acc: 0.3047 | test_loss: 1.0838 | test_acc: 0.2604\n",
            "Epoch: 6 | train_loss: 1.0859 | train_acc: 0.4258 | test_loss: 1.0722 | test_acc: 0.2604\n",
            "Epoch: 7 | train_loss: 1.0868 | train_acc: 0.3047 | test_loss: 1.0692 | test_acc: 0.2604\n",
            "Epoch: 8 | train_loss: 1.0669 | train_acc: 0.3203 | test_loss: 1.0442 | test_acc: 0.3419\n",
            "Epoch: 9 | train_loss: 1.0456 | train_acc: 0.3164 | test_loss: 1.0525 | test_acc: 0.4924\n",
            "Epoch: 10 | train_loss: 1.0507 | train_acc: 0.4023 | test_loss: 0.9974 | test_acc: 0.3920\n",
            "Epoch: 11 | train_loss: 0.9558 | train_acc: 0.6523 | test_loss: 1.0248 | test_acc: 0.3011\n",
            "Epoch: 12 | train_loss: 0.9108 | train_acc: 0.6289 | test_loss: 0.9772 | test_acc: 0.4735\n",
            "Epoch: 13 | train_loss: 0.9418 | train_acc: 0.4883 | test_loss: 1.0118 | test_acc: 0.4848\n",
            "Epoch: 14 | train_loss: 0.9289 | train_acc: 0.5469 | test_loss: 1.0024 | test_acc: 0.4337\n",
            "Epoch: 15 | train_loss: 0.9017 | train_acc: 0.6133 | test_loss: 1.0280 | test_acc: 0.4337\n",
            "Epoch: 16 | train_loss: 0.8258 | train_acc: 0.6523 | test_loss: 1.0191 | test_acc: 0.4432\n",
            "Epoch: 17 | train_loss: 0.8361 | train_acc: 0.5430 | test_loss: 1.0266 | test_acc: 0.4223\n",
            "Epoch: 18 | train_loss: 0.8383 | train_acc: 0.5664 | test_loss: 1.0191 | test_acc: 0.4536\n",
            "Epoch: 19 | train_loss: 0.8303 | train_acc: 0.6641 | test_loss: 0.9649 | test_acc: 0.5142\n",
            "Epoch: 20 | train_loss: 0.8226 | train_acc: 0.6484 | test_loss: 0.9713 | test_acc: 0.5152\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'train_loss': [1.107267677783966,\n",
              "  1.1030047684907913,\n",
              "  1.092466562986374,\n",
              "  1.099650353193283,\n",
              "  1.0965719819068909,\n",
              "  1.085919350385666,\n",
              "  1.0867832601070404,\n",
              "  1.0668695718050003,\n",
              "  1.045585185289383,\n",
              "  1.0507371053099632,\n",
              "  0.9558431878685951,\n",
              "  0.910836398601532,\n",
              "  0.9417855143547058,\n",
              "  0.9289010614156723,\n",
              "  0.9017343446612358,\n",
              "  0.8258182927966118,\n",
              "  0.8360626175999641,\n",
              "  0.8383419886231422,\n",
              "  0.8302755132317543,\n",
              "  0.8226142972707748],\n",
              " 'train_acc': [0.28125,\n",
              "  0.296875,\n",
              "  0.42578125,\n",
              "  0.3046875,\n",
              "  0.3046875,\n",
              "  0.42578125,\n",
              "  0.3046875,\n",
              "  0.3203125,\n",
              "  0.31640625,\n",
              "  0.40234375,\n",
              "  0.65234375,\n",
              "  0.62890625,\n",
              "  0.48828125,\n",
              "  0.546875,\n",
              "  0.61328125,\n",
              "  0.65234375,\n",
              "  0.54296875,\n",
              "  0.56640625,\n",
              "  0.6640625,\n",
              "  0.6484375],\n",
              " 'test_loss': [1.087166468302409,\n",
              "  1.1062618494033813,\n",
              "  1.0998746156692505,\n",
              "  1.0982983509699504,\n",
              "  1.0837994813919067,\n",
              "  1.0722003777821858,\n",
              "  1.0692476828893025,\n",
              "  1.04424915711085,\n",
              "  1.0525358319282532,\n",
              "  0.9974040985107422,\n",
              "  1.0247993469238281,\n",
              "  0.9771521091461182,\n",
              "  1.0118384162584941,\n",
              "  1.0024203856786091,\n",
              "  1.02800057331721,\n",
              "  1.019060989220937,\n",
              "  1.0265727241834004,\n",
              "  1.0190719763437908,\n",
              "  0.9649165074030558,\n",
              "  0.9713080922762553],\n",
              " 'test_acc': [0.5416666666666666,\n",
              "  0.2604166666666667,\n",
              "  0.2604166666666667,\n",
              "  0.2604166666666667,\n",
              "  0.2604166666666667,\n",
              "  0.2604166666666667,\n",
              "  0.2604166666666667,\n",
              "  0.3418560606060606,\n",
              "  0.49242424242424243,\n",
              "  0.3920454545454546,\n",
              "  0.30113636363636365,\n",
              "  0.47348484848484845,\n",
              "  0.48484848484848486,\n",
              "  0.43371212121212127,\n",
              "  0.43371212121212127,\n",
              "  0.4431818181818182,\n",
              "  0.42234848484848486,\n",
              "  0.45359848484848486,\n",
              "  0.5142045454545454,\n",
              "  0.5151515151515151]}"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Create a function to save a model"
      ],
      "metadata": {
        "id": "AdPDQ-zWgCoH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "def save_model(model: torch.nn.Module,\n",
        "               target_dir: str,\n",
        "               model_name: str):\n",
        "  \"\"\"Saves a PyTorch model to target_dir.\n",
        "\n",
        "  Args:\n",
        "    model: A target PyTorch model to save.\n",
        "    target_dir: A directory for saving the model to.\n",
        "    model_name: A filename for the saved model. Should include\n",
        "      either \".pth\" or \".pt\" as the file extention.\n",
        "\n",
        "  Example usage:\n",
        "    save_model(model=model_0,\n",
        "                target_dir=\"models\",\n",
        "                model_name=\"example_model_name\")\n",
        "  \"\"\"\n",
        "  # create target directory\n",
        "  target_dir_path = Path(target_dir)\n",
        "  target_dir_path.mkdir(parents=True,\n",
        "                        exist_ok=True)\n",
        "\n",
        "  # create model save path\n",
        "  assert model_name.endswith(\".pth\") or model_name.endswith('.pt'), \\\n",
        "  \"model_name should end with '.pth' or '.pt'\"\n",
        "  model_save_path = target_dir_path / model_name\n",
        "\n",
        "  # save the model state_dict\n",
        "  print(f\"[INFO] Saving model to: {model_save_path}\")\n",
        "  torch.save(obj=model.state_dict(),\n",
        "             f=model_save_path)"
      ],
      "metadata": {
        "id": "eBqgCaHS-u0z"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile going_modular/utils.py\n",
        "\"\"\"\n",
        "File containing various utility functions for PyTorch model training.\n",
        "\"\"\"\n",
        "import torch\n",
        "from pathlib import Path\n",
        "\n",
        "def save_model(model: torch.nn.Module,\n",
        "               target_dir: str,\n",
        "               model_name: str):\n",
        "  \"\"\"\n",
        "  Saves a PyTorch model to target_dir.\n",
        "\n",
        "  Args:\n",
        "    model: A target PyTorch model to save.\n",
        "    target_dir: A directory for saving the model to.\n",
        "    model_name: A filename for the saved model. Should include\n",
        "      either \".pth\" or \".pt\" as the file extention.\n",
        "\n",
        "  Example usage:\n",
        "    save_model(model=model_0,\n",
        "                target_dir=\"models\",\n",
        "                model_name=\"example_model_name\")\n",
        "  \"\"\"\n",
        "  # Create target directory\n",
        "  target_dir_path = Path(target_dir)\n",
        "  target_dir_path.mkdir(parents=True,\n",
        "                        exist_ok=True)\n",
        "\n",
        "  # Create model save path\n",
        "  assert model_name.endswith(\".pth\") or model_name.endswith(\".pt\"), \\\n",
        "  \"model_name must end with '.pth' or '.pt'\"\n",
        "  model_save_path = target_dir_path / model_name\n",
        "\n",
        "  # save model state_dict\n",
        "  print(f'[INFO] Saving model to {model_save_path}')\n",
        "  torch.save(obj=model.state_dict(),\n",
        "             f=model_save_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ruMXsbzAZTs",
        "outputId": "453bf04b-dec7-4f18-80a9-5356b1114dfc"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting going_modular/utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from going_modular import utils"
      ],
      "metadata": {
        "id": "I6TvkKRbBwH7"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "utils.save_model(model=model_0,\n",
        "                 target_dir='models',\n",
        "                 model_name='model_0_tinyvgg.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WrTG1iwpBypN",
        "outputId": "1a033009-9e31-4abc-a564-a1fa8bab51ff"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Saving model to models/model_0_tinyvgg.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train, evaluate and save model in `train.py`"
      ],
      "metadata": {
        "id": "bDhnkxeHB6cq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile going_modular/train.py\n",
        "\"\"\"\n",
        "Trains a PyTorch image classification model using device-agnostic code.\n",
        "\"\"\"\n",
        "import torch\n",
        "import os\n",
        "from torchvision import transforms\n",
        "import data_setup, model_builder, engine, utils\n",
        "from timeit import default_timer as timer\n",
        "\n",
        "# Setup hyperparameters\n",
        "EPOCHS = 5\n",
        "BATCH_SIZE = 32\n",
        "HIDDEN_UNITS = 10\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "# Setup directories\n",
        "train_dir = \"data/pizza_steak_sushi/train\"\n",
        "test_dir = \"data/pizza_steak_sushi/test\"\n",
        "\n",
        "# Setup device agnostic\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Create transform\n",
        "data_transform = transforms.Compose([\n",
        "    transforms.Resize(size=(64, 64)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "\n",
        "# Create DataLoaders and get class_names\n",
        "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n",
        "    train_dir=train_dir,\n",
        "    test_dir=test_dir,\n",
        "    transform=data_transform,\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "\n",
        "# Create model\n",
        "model = model_builder.TinyVGG(input_shape=3,\n",
        "                              hidden_units=HIDDEN_UNITS,\n",
        "                              output_shape=len(class_names))\n",
        "\n",
        "# Setup loss and optimizer\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(params=model.parameters(),\n",
        "                             lr=LEARNING_RATE)\n",
        "\n",
        "start_time = timer()\n",
        "# Start training with help from engine.py\n",
        "engine.train(model=model,\n",
        "             train_dataloader=train_dataloader,\n",
        "             test_dataloader=test_dataloader,\n",
        "             loss_fn=loss_fn,\n",
        "             optimizer=optimizer,\n",
        "             epochs=EPOCHS,\n",
        "             device=device)\n",
        "end_time = timer()\n",
        "print(f\"[INFO] Total training time: {end_time - start_time} seconds\")\n",
        "\n",
        "# Save model\n",
        "utils.save_model(model=model,\n",
        "                 target_dir='models',\n",
        "                 model_name='model_0_tinyvgg1.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dab6-n7jCpx3",
        "outputId": "895007a0-9a1f-4041-94f9-ce70abfa61fb"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting going_modular/train.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python going_modular/train.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMAwyt3fFIoe",
        "outputId": "602acbd7-33bb-4d02-e4aa-7b08dd24c3b9"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 | train_loss: 1.1028 | train_acc: 0.3633 | test_loss: 1.0854 | test_acc: 0.5417\n",
            "Epoch: 2 | train_loss: 1.0871 | train_acc: 0.4023 | test_loss: 1.0597 | test_acc: 0.5417\n",
            "Epoch: 3 | train_loss: 1.0819 | train_acc: 0.4023 | test_loss: 1.0440 | test_acc: 0.5417\n",
            "Epoch: 4 | train_loss: 1.0700 | train_acc: 0.4023 | test_loss: 1.0348 | test_acc: 0.5417\n",
            "Epoch: 5 | train_loss: 1.0795 | train_acc: 0.2812 | test_loss: 1.0257 | test_acc: 0.5322\n",
            "[INFO] Total training time: 10.64916669199988 seconds\n",
            "[INFO] Saving model to models/model_0_tinyvgg1.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HAAkea8RFhkJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}